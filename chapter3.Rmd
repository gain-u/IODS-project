# Student performance (incl. alcohol consumption)

This is an exercise on data from two questionnaires related to student performance (especially including alcohol consumption). More information about the data sets  can be found in the link below.

<https://archive.ics.uci.edu/ml/datasets/Student+Performance>

The data have been adjusted as follow :  
* The variables not used for joining the two data have been combined by averaging (including the grade variables)  
* 'alc_use' is the average of 'Dalc' and 'Walc'  
* 'high_use' is TRUE if 'alc_use' is higher than 2 and FALSE otherwise  
```{r}
#Read the analysis dataset from data folder
alc <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt", header = TRUE, sep =",")
colnames(alc)
```


The purpose of the analysis is to study the relationships between high/low alcohol consumption and some of the other variables in the data. To do this, 4 interesting variables have been chosen. The hypothesis for the chosen variables and  about their relationships with alcohol consumption are as follows:
Variables 1-4 (listed below) with values hig, A, high and no will increase alcohol consumption

1.goout - going out with friends (numeric: from 1 - very low to 5 - very high):high  
2.Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart):A  
3.freetime - free time after school (numeric: from 1 - very low to 5 - very high):high  
4.activities - extra-curricular activities (binary: yes or no):no  
  
  The distribution of chosen variables and relationships with alcohol consumption is explored with cross-tabulations, bar plot and box plots.  
```{r}
# access the dplyr library
library(dplyr)

#check for chosen variables
#alc_own <- select(alc, "goout","Pstatus","freetime","activities")
#alc_own

# access the 'tidyverse' packages dplyr and ggplot2
library(dplyr); library(ggplot2)

# initialize a plot of alcohol use
g1 <- ggplot(data = alc, aes(x = alc_use, fill = goout))

# define the plot as a bar plot and draw it
#g1 + geom_bar()

# initialize a plot of 'high_use'
g2 <- ggplot(alc, aes(high_use))

# draw a bar plot of high_use by sex
g2 + facet_wrap("goout") + geom_bar()
g2 + facet_wrap("Pstatus") + geom_bar()
g2 + facet_wrap("freetime") + geom_bar()
g2 + facet_wrap("activities") + geom_bar()
```
  
The findings of the exploration and the hypothesis seems, all of chosen variables shows in bar plots that hypothesis values are inverse (meaning that high use of alcohol is  more cases false than true). 

The logistic regression is used explore the relationship between the chosen variables and the binary high/low alcohol consumption variable as the target variable. The summary of the fitted model is as follow:  
```{r}
# find the model with glm()
m <- glm(high_use ~ goout + Pstatus + freetime + activities, data = alc, family = "binomial")

# print out a summary of the model
summary(m)

# print out the coefficients of the model
coef(m)

# compute odds ratios (OR)
OR <- coef(m) %>% exp

# compute confidence intervals (CI)
CI <- confint(m) %>% exp

# print out the odds ratios with their confidence intervals
cbind(OR, CI)
```
Further  the coefficients of the model as odds ratios and provide confidence intervals for them:  
Interpretation of the results and comparison them to previously stated hypothesis.
Only goout has very low p value, therefore there is statistical relationship between alcohol high_use and goout variables.It has odds ratio = 2,0635 and  confidence interval 2,5% 1,640; 97,5 2,632.

The variables which, according to logistic regression model, had a statistical relationship with high/low alcohol consumption, explore the predictive power of the model. Here is provided a 2x2 cross tabulation of predictions versus the actual values and  displayed a graphic visualizing both the actual values and the predictions.
```{r}
# predict() the probability of high_use
probabilities <- predict(m, type = "response")

# add the predicted probabilities to 'alc'
alc <- mutate(alc, probability = probabilities)

# use the probabilities to make a prediction of high_use
alc <- mutate(alc, prediction = probability > 0.5)

# see the last ten original classes, predicted probabilities, and class predictions
select(alc, goout, high_use, probability, prediction) %>% tail(10)

# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction)

# access dplyr and ggplot2
library(dplyr); library(ggplot2)

# initialize a plot of 'high_use' versus 'probability' in 'alc'
g <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))

# define the geom as points and draw the plot
g + geom_point()

# tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$prediction) %>% prop.table %>% addmargins
```

The total proportion of inaccurately classified individuals (= the training error)  is computed the results can be find here:
```{r}
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = 0)
```
Smaller the number of loss function output is => better the prediction result.

The performance of the model is robust verifiable way to find high performance result than achieved by some simple guessing strategy. 

Further 10-fold cross-validation on the model.  
```{r}

```


Does the model have better test set performance (smaller prediction error using 10-fold cross-validation) compared to the model introduced in DataCamp (which had about 0.26 error). Could you find such a model?  


The cross-validation to compare the performance of different logistic regression models (= different sets of predictors). The task has been started with a very high number of predictors and explored the changes in the training and testing errors as went to models with less predictors.   
```{r}

```

Drawed a graph to display the trends of both training and testing errors by the number of predictors in the model.   
```{r}

```

